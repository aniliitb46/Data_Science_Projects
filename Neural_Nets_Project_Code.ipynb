{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Neural_Nets_Project_Starter_Code_Final_Anil.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"neg58q6WNP77"},"source":["# Gesture Recognition\n","In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXGhTQq7Q07V","executionInfo":{"status":"ok","timestamp":1624038274977,"user_tz":-330,"elapsed":517,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"02c360cb-ec36-4c4f-858f-1038abb2a135"},"source":["## Checking the GPU configuration\n","\n","!nvidia-smi "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Jun 18 17:44:34 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ui-V7KE-Q3Gx","executionInfo":{"status":"ok","timestamp":1624038280247,"user_tz":-330,"elapsed":3514,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"71c1a59d-b7dc-4f6d-9536-19b0c8b166d7"},"source":["! pip install scipy==1.1.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"28tfAq60NP7-"},"source":["import numpy as np\n","import os\n","from scipy.misc import imread, imresize\n","import datetime\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import abc\n","from sys import getsizeof"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqhYcLNlNP7_"},"source":["We set the random seed so that the results don't vary drastically."]},{"cell_type":"code","metadata":{"id":"WMwG0pDoNP7_"},"source":["np.random.seed(30)\n","import random as rn\n","rn.seed(30)\n","from keras import backend as K\n","import tensorflow as tf\n","tf.random.set_seed(30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mfv-qYbhRPlF"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96e94HrKSNYt"},"source":["from keras.models import Sequential, Model\n","from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n","from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n","from keras.layers.recurrent import LSTM\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras import optimizers\n","from keras.layers import Dropout"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eyio1Ds4NP7_"},"source":["In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNKTut95NP7_","executionInfo":{"status":"ok","timestamp":1624038299082,"user_tz":-330,"elapsed":456,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"6180d4f1-c4f7-4221-e26f-3b2c73fd0661"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRi53J55Rh3x","executionInfo":{"status":"ok","timestamp":1624038356835,"user_tz":-330,"elapsed":10082,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"8d21b015-322b-4fab-9bb5-a22342fd2146"},"source":["\n","!unzip \"gdrive/My Drive/Colab Notebooks/Project_data.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  gdrive/My Drive/Colab Notebooks/Project_data.zip\n","replace Project_data/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: NO\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K9VfI6wXR1Fk"},"source":["project_folder='Project_data'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWzuBYmXNP8A"},"source":["## Generator\n","This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."]},{"cell_type":"code","metadata":{"id":"wwXiN7J6NP8A"},"source":["class ModelBuilder(metaclass= abc.ABCMeta):\n","    # initialisng the path where project data resides\n","    def initialize_path(self,project_folder):\n","        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n","        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n","        self.train_path = project_folder + '/' + 'train'\n","        self.val_path =  project_folder + '/' + 'val'\n","        self.num_train_sequences = len(self.train_doc)\n","        self.num_val_sequences = len(self.val_doc)\n","    # initialising the image properties    \n","    def initialize_image_properties(self,image_height=100,image_width=100):\n","        self.image_height=image_height\n","        self.image_width=image_width\n","        self.channels=3\n","        self.num_classes=5\n","        self.total_frames=30\n","    # initialising the batch size, frames to sample and the no. of epochs\n","    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n","        self.frames_to_sample=frames_to_sample\n","        self.batch_size=batch_size\n","        self.num_epochs=num_epochs\n","        \n","    # MOST IMPORTANT PART HERE - The generator function        \n","    def generator(self,source_path, folder_list, augment=False):\n","        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n","        batch_size=self.batch_size\n","        while True:\n","            t = np.random.permutation(folder_list)\n","            num_batches = len(t)//batch_size\n","        \n","            for batch in range(num_batches): \n","                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n","                yield batch_data, batch_labels \n","\n","            remaining_seq=len(t)%batch_size\n","        \n","            if (remaining_seq != 0):\n","                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n","                yield batch_data, batch_labels \n","    \n","    \n","    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n","    \n","        seq_len = remaining_seq if remaining_seq else batch_size\n","    \n","        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n","        batch_labels = np.zeros((seq_len,self.num_classes)) \n","    \n","        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n","\n","        \n","        for folder in range(seq_len): \n","            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n","            for idx,item in enumerate(img_idx):\n","                #performing image reading and resizing\n","                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                image_resized=imresize(image,(self.image_height,self.image_width,3))\n","            \n","                #normalizing the images\n","                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n","                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n","                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n","            \n","                if (augment):\n","                    shifted = cv2.warpAffine(image, \n","                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n","                                            (image.shape[1], image.shape[0]))\n","                    \n","                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n","\n","                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n","                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n","                    # cropping the images to have the targeted gestures and remove the noise from the images.\n","                    cropped=shifted[x0:x1,y0:y1,:]\n","                    \n","                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n","                    \n","                    #shifted = cv2.warpAffine(image_resized, \n","                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n","                    #                        (image_resized.shape[1], image_resized.shape[0]))\n","            \n","                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n","                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n","                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n","                \n","            \n","            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            \n","    \n","        if (augment):\n","            batch_data=np.concatenate([batch_data,batch_data_aug])\n","            batch_labels=np.concatenate([batch_labels,batch_labels])\n","\n","        \n","        return(batch_data,batch_labels)\n","    \n","    \n","    def train_model(self, model, augment_data=False):\n","        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n","        val_generator = self.generator(self.val_path, self.val_doc)\n","\n","        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n","    \n","        if not os.path.exists(model_name):\n","            os.mkdir(model_name)\n","        \n","        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n","        \n","        earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n","        callbacks_list = [checkpoint, LR, earlystop]\n","\n","        if (self.num_train_sequences%self.batch_size) == 0:\n","            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n","        else:\n","            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n","\n","        if (self.num_val_sequences%self.batch_size) == 0:\n","            validation_steps = int(self.num_val_sequences/self.batch_size)\n","        else:\n","            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n","    \n","        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n","                            callbacks=callbacks_list, validation_data=val_generator, \n","                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n","        return history\n","\n","        \n","    @abc.abstractmethod\n","    def define_model(self):\n","        pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkFYZgo8NP8B"},"source":["Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."]},{"cell_type":"markdown","metadata":{"id":"egDDhhR4NP8B"},"source":["## Model\n","Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."]},{"cell_type":"markdown","metadata":{"id":"V32bqZy-TrqI"},"source":["### **Sample Model**"]},{"cell_type":"code","metadata":{"id":"UE3fpTY8TqMM"},"source":["class ModelConv3D1(ModelBuilder):\n","    \n","    def define_model(self):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, (3, 3, 3), padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dense(128,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(64,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.25))\n","\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam()\n","        #optimiser = 'sgd'\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gc1zluqZNP8E","executionInfo":{"status":"ok","timestamp":1624038373174,"user_tz":-330,"elapsed":3074,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"77506780-9e34-4dc1-e3f9-aec7228b9cf1"},"source":["conv_3d1=ModelConv3D1()\n","conv_3d1.initialize_path(project_folder)\n","conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n","conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n","conv_3d1_model=conv_3d1.define_model()\n","conv_3d1_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n","_________________________________________________________________\n","activation (Activation)      (None, 30, 160, 160, 16)  0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n","_________________________________________________________________\n","max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n","_________________________________________________________________\n","conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n","_________________________________________________________________\n","max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n","_________________________________________________________________\n","conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n","_________________________________________________________________\n","conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 12800)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               1638528   \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 128)               512       \n","_________________________________________________________________\n","dropout (Dropout)            (None, 128)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 64)                256       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 5)                 325       \n","=================================================================\n","Total params: 1,736,389\n","Trainable params: 1,735,525\n","Non-trainable params: 864\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gqSUgHsET99s"},"source":["## **Model 1**\n","### **Base Model - Batch Size = 40 and No. of Epochs = 15**"]},{"cell_type":"code","metadata":{"id":"b1uj-TKiT5Uy"},"source":["class ModelConv3D1(ModelBuilder):\n","    \n","    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam()\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULvUcZ22ULul","executionInfo":{"status":"ok","timestamp":1624038378808,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"8fe50304-0229-4f06-9a20-22c1795b98a6"},"source":["conv_3d1=ModelConv3D1()\n","conv_3d1.initialize_path(project_folder)\n","conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n","conv_3d1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n","conv_3d1_model=conv_3d1.define_model()\n","conv_3d1_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_4 (Conv3D)            (None, 20, 160, 160, 16)  1312      \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 20, 160, 160, 16)  0         \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 20, 160, 160, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_4 (MaxPooling3 (None, 10, 80, 80, 16)    0         \n","_________________________________________________________________\n","conv3d_5 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 10, 80, 80, 32)    0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 10, 80, 80, 32)    128       \n","_________________________________________________________________\n","max_pooling3d_5 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n","_________________________________________________________________\n","conv3d_6 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 5, 40, 40, 64)     0         \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 5, 40, 40, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_6 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n","_________________________________________________________________\n","conv3d_7 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 2, 20, 20, 128)    0         \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 2, 20, 20, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_7 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 12800)             0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 64)                819264    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 64)                256       \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 64)                256       \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 5)                 325       \n","=================================================================\n","Total params: 1,117,061\n","Trainable params: 1,116,325\n","Non-trainable params: 736\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OMtp1cWjUnvJ"},"source":["from keras.callbacks import EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtL-p8TWURn1","executionInfo":{"status":"ok","timestamp":1624039181347,"user_tz":-330,"elapsed":799772,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"8376c8ea-7544-43a4-cc1d-ac650b1da54e"},"source":["print(\"Total Params:\", conv_3d1_model.count_params())\n","history_model1 = conv_3d1.train_model(conv_3d1_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 1117061\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/15\n","17/17 [==============================] - 112s 5s/step - loss: 1.7487 - categorical_accuracy: 0.3528 - val_loss: 1.6228 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00001: val_loss improved from inf to 1.62282, saving model to model_init_2021-06-1817_46_21.743083/model-00001-1.52869-0.44193-1.62282-0.18000.h5\n","Epoch 2/15\n","17/17 [==============================] - 71s 4s/step - loss: 1.0351 - categorical_accuracy: 0.5882 - val_loss: 2.4282 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00002: val_loss did not improve from 1.62282\n","Epoch 3/15\n","17/17 [==============================] - 66s 4s/step - loss: 0.6423 - categorical_accuracy: 0.7598 - val_loss: 3.1685 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00003: val_loss did not improve from 1.62282\n","Epoch 4/15\n","17/17 [==============================] - 68s 4s/step - loss: 0.4870 - categorical_accuracy: 0.8342 - val_loss: 3.4443 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00004: val_loss did not improve from 1.62282\n","Epoch 5/15\n","17/17 [==============================] - 68s 4s/step - loss: 0.4047 - categorical_accuracy: 0.8422 - val_loss: 4.6530 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00005: val_loss did not improve from 1.62282\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 6/15\n","17/17 [==============================] - 63s 4s/step - loss: 0.3117 - categorical_accuracy: 0.8882 - val_loss: 4.6962 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00006: val_loss did not improve from 1.62282\n","Epoch 7/15\n","17/17 [==============================] - 72s 4s/step - loss: 0.2493 - categorical_accuracy: 0.9280 - val_loss: 5.1425 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00007: val_loss did not improve from 1.62282\n","Epoch 8/15\n","17/17 [==============================] - 72s 4s/step - loss: 0.2419 - categorical_accuracy: 0.9261 - val_loss: 5.2808 - val_categorical_accuracy: 0.2000\n","\n","Epoch 00008: val_loss did not improve from 1.62282\n","Epoch 9/15\n","17/17 [==============================] - 63s 4s/step - loss: 0.1965 - categorical_accuracy: 0.9554 - val_loss: 5.0334 - val_categorical_accuracy: 0.2700\n","\n","Epoch 00009: val_loss did not improve from 1.62282\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 10/15\n","17/17 [==============================] - 73s 5s/step - loss: 0.1913 - categorical_accuracy: 0.9632 - val_loss: 5.6794 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00010: val_loss did not improve from 1.62282\n","Epoch 11/15\n","17/17 [==============================] - 67s 4s/step - loss: 0.1619 - categorical_accuracy: 0.9701 - val_loss: 5.5688 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00011: val_loss did not improve from 1.62282\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GheyUQLyaiwu"},"source":["### **Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbZToRLAapZ2","executionInfo":{"status":"ok","timestamp":1624039207395,"user_tz":-330,"elapsed":645,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"6df71a08-2dad-4275-b820-bf4b9d076a7b"},"source":["conv_3d2=ModelConv3D1()\n","conv_3d2.initialize_path(project_folder)\n","conv_3d2.initialize_image_properties(image_height=160,image_width=160)\n","conv_3d2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n","conv_3d2_model=conv_3d2.define_model(dense_neurons=256,dropout=0.5)\n","conv_3d2_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_8 (Conv3D)            (None, 20, 160, 160, 16)  1312      \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 20, 160, 160, 16)  0         \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 20, 160, 160, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_8 (MaxPooling3 (None, 10, 80, 80, 16)    0         \n","_________________________________________________________________\n","conv3d_9 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n","_________________________________________________________________\n","activation_9 (Activation)    (None, 10, 80, 80, 32)    0         \n","_________________________________________________________________\n","batch_normalization_13 (Batc (None, 10, 80, 80, 32)    128       \n","_________________________________________________________________\n","max_pooling3d_9 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n","_________________________________________________________________\n","conv3d_10 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n","_________________________________________________________________\n","activation_10 (Activation)   (None, 5, 40, 40, 64)     0         \n","_________________________________________________________________\n","batch_normalization_14 (Batc (None, 5, 40, 40, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_10 (MaxPooling (None, 2, 20, 20, 64)     0         \n","_________________________________________________________________\n","conv3d_11 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n","_________________________________________________________________\n","activation_11 (Activation)   (None, 2, 20, 20, 128)    0         \n","_________________________________________________________________\n","batch_normalization_15 (Batc (None, 2, 20, 20, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_11 (MaxPooling (None, 1, 10, 10, 128)    0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 12800)             0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 256)               3277056   \n","_________________________________________________________________\n","batch_normalization_16 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","batch_normalization_17 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 5)                 1285      \n","=================================================================\n","Total params: 3,638,981\n","Trainable params: 3,637,477\n","Non-trainable params: 1,504\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdy7pGcnasjN","executionInfo":{"status":"ok","timestamp":1624040719743,"user_tz":-330,"elapsed":1508718,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"be021951-ed16-461f-e827-f5aaa3f3a272"},"source":["print(\"Total Params:\", conv_3d2_model.count_params())\n","history_model2=conv_3d2.train_model(conv_3d2_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 3638981\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/25\n","34/34 [==============================] - 133s 4s/step - loss: 2.1587 - categorical_accuracy: 0.3323 - val_loss: 2.6599 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00001: val_loss improved from inf to 2.65990, saving model to model_init_2021-06-1818_00_11.148119/model-00001-1.97006-0.36199-2.65990-0.18000.h5\n","Epoch 2/25\n","34/34 [==============================] - 132s 4s/step - loss: 1.5350 - categorical_accuracy: 0.4664 - val_loss: 4.4153 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00002: val_loss did not improve from 2.65990\n","Epoch 3/25\n","34/34 [==============================] - 139s 4s/step - loss: 1.2065 - categorical_accuracy: 0.5406 - val_loss: 4.8322 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00003: val_loss did not improve from 2.65990\n","Epoch 4/25\n","34/34 [==============================] - 140s 4s/step - loss: 1.1368 - categorical_accuracy: 0.5731 - val_loss: 4.3759 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00004: val_loss did not improve from 2.65990\n","Epoch 5/25\n","34/34 [==============================] - 133s 4s/step - loss: 0.8675 - categorical_accuracy: 0.6675 - val_loss: 5.0634 - val_categorical_accuracy: 0.2600\n","\n","Epoch 00005: val_loss did not improve from 2.65990\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 6/25\n","34/34 [==============================] - 133s 4s/step - loss: 0.7847 - categorical_accuracy: 0.6961 - val_loss: 4.9228 - val_categorical_accuracy: 0.2200\n","\n","Epoch 00006: val_loss did not improve from 2.65990\n","Epoch 7/25\n","34/34 [==============================] - 140s 4s/step - loss: 0.6863 - categorical_accuracy: 0.7537 - val_loss: 4.3066 - val_categorical_accuracy: 0.2700\n","\n","Epoch 00007: val_loss did not improve from 2.65990\n","Epoch 8/25\n","34/34 [==============================] - 141s 4s/step - loss: 0.6378 - categorical_accuracy: 0.7589 - val_loss: 4.4627 - val_categorical_accuracy: 0.2200\n","\n","Epoch 00008: val_loss did not improve from 2.65990\n","Epoch 9/25\n","34/34 [==============================] - 136s 4s/step - loss: 0.5990 - categorical_accuracy: 0.7680 - val_loss: 4.0163 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00009: val_loss did not improve from 2.65990\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 10/25\n","34/34 [==============================] - 135s 4s/step - loss: 0.5809 - categorical_accuracy: 0.7867 - val_loss: 3.8022 - val_categorical_accuracy: 0.2700\n","\n","Epoch 00010: val_loss did not improve from 2.65990\n","Epoch 11/25\n","34/34 [==============================] - 142s 4s/step - loss: 0.5147 - categorical_accuracy: 0.8139 - val_loss: 3.2699 - val_categorical_accuracy: 0.2900\n","\n","Epoch 00011: val_loss did not improve from 2.65990\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5jmtVmn1fgDG"},"source":["### **Model 3 - Reduce filter size to (2,2,2) and image res to 120 x 120**"]},{"cell_type":"code","metadata":{"id":"R0I4G5uifkOV"},"source":["class ModelConv3D3(ModelBuilder):\n","    \n","    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam(lr=0.0002)\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGGOtNkKfwmq","executionInfo":{"status":"ok","timestamp":1624040742354,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"9810eb4e-dfa4-4bcf-c723-53d71351d3d6"},"source":["conv_3d3=ModelConv3D3()\n","conv_3d3.initialize_path(project_folder)\n","conv_3d3.initialize_image_properties(image_height=120,image_width=120)\n","conv_3d3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n","conv_3d3_model=conv_3d3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n","conv_3d3_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_12 (Conv3D)           (None, 16, 120, 120, 16)  400       \n","_________________________________________________________________\n","activation_12 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_18 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_12 (MaxPooling (None, 8, 60, 60, 16)     0         \n","_________________________________________________________________\n","conv3d_13 (Conv3D)           (None, 8, 60, 60, 32)     4128      \n","_________________________________________________________________\n","activation_13 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_19 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","max_pooling3d_13 (MaxPooling (None, 4, 30, 30, 32)     0         \n","_________________________________________________________________\n","conv3d_14 (Conv3D)           (None, 4, 30, 30, 64)     16448     \n","_________________________________________________________________\n","activation_14 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_20 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_14 (MaxPooling (None, 2, 15, 15, 64)     0         \n","_________________________________________________________________\n","conv3d_15 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n","_________________________________________________________________\n","activation_15 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_21 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_15 (MaxPooling (None, 1, 7, 7, 128)      0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 256)               1605888   \n","_________________________________________________________________\n","batch_normalization_22 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 256)               65792     \n","_________________________________________________________________\n","batch_normalization_23 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 5)                 1285      \n","=================================================================\n","Total params: 1,762,613\n","Trainable params: 1,761,109\n","Non-trainable params: 1,504\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25Wml5tDxYwy","executionInfo":{"status":"ok","timestamp":1624041653025,"user_tz":-330,"elapsed":908629,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"eada4fbd-fa46-45ab-be21-3fd6aeb2a877"},"source":["print(\"Total Params:\", conv_3d3_model.count_params())\n","history_model3=conv_3d3.train_model(conv_3d3_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 1762613\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/30\n","23/23 [==============================] - 89s 4s/step - loss: 2.2874 - categorical_accuracy: 0.2884 - val_loss: 2.0138 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00001: val_loss improved from inf to 2.01378, saving model to model_init_2021-06-1818_25_44.125859/model-00001-2.13579-0.32805-2.01378-0.16000.h5\n","Epoch 2/30\n","23/23 [==============================] - 82s 4s/step - loss: 1.5212 - categorical_accuracy: 0.4801 - val_loss: 3.4138 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00002: val_loss did not improve from 2.01378\n","Epoch 3/30\n","23/23 [==============================] - 81s 4s/step - loss: 1.3558 - categorical_accuracy: 0.5308 - val_loss: 5.1972 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00003: val_loss did not improve from 2.01378\n","Epoch 4/30\n","23/23 [==============================] - 82s 4s/step - loss: 1.2491 - categorical_accuracy: 0.5709 - val_loss: 6.7852 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00004: val_loss did not improve from 2.01378\n","Epoch 5/30\n","23/23 [==============================] - 80s 4s/step - loss: 1.0410 - categorical_accuracy: 0.6308 - val_loss: 8.0730 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00005: val_loss did not improve from 2.01378\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n","Epoch 6/30\n","23/23 [==============================] - 81s 4s/step - loss: 1.0797 - categorical_accuracy: 0.6115 - val_loss: 9.3738 - val_categorical_accuracy: 0.1200\n","\n","Epoch 00006: val_loss did not improve from 2.01378\n","Epoch 7/30\n","23/23 [==============================] - 83s 4s/step - loss: 0.8751 - categorical_accuracy: 0.6863 - val_loss: 10.0485 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00007: val_loss did not improve from 2.01378\n","Epoch 8/30\n","23/23 [==============================] - 81s 4s/step - loss: 0.9673 - categorical_accuracy: 0.6481 - val_loss: 9.8726 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00008: val_loss did not improve from 2.01378\n","Epoch 9/30\n","23/23 [==============================] - 82s 4s/step - loss: 0.8648 - categorical_accuracy: 0.6810 - val_loss: 10.4899 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00009: val_loss did not improve from 2.01378\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n","Epoch 10/30\n","23/23 [==============================] - 81s 4s/step - loss: 0.8521 - categorical_accuracy: 0.6785 - val_loss: 9.9133 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00010: val_loss did not improve from 2.01378\n","Epoch 11/30\n","23/23 [==============================] - 82s 4s/step - loss: 0.8470 - categorical_accuracy: 0.6887 - val_loss: 9.7101 - val_categorical_accuracy: 0.1400\n","\n","Epoch 00011: val_loss did not improve from 2.01378\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ALpkksCC1mvE"},"source":["### **Model 4 - Adding more layers**"]},{"cell_type":"code","metadata":{"id":"knj1KeCNxZUF"},"source":["class ModelConv3D4(ModelBuilder):\n","    \n","    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        \n","\n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam()\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1vG0DwR10AK","executionInfo":{"status":"ok","timestamp":1624041708418,"user_tz":-330,"elapsed":578,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"6dc46b97-df2f-4bc4-f1d7-8dd37653a231"},"source":["conv_3d4=ModelConv3D4()\n","conv_3d4.initialize_path(project_folder)\n","conv_3d4.initialize_image_properties(image_height=120,image_width=120)\n","conv_3d4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n","conv_3d4_model=conv_3d4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n","conv_3d4_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_16 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n","_________________________________________________________________\n","activation_16 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_24 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","conv3d_17 (Conv3D)           (None, 16, 120, 120, 16)  6928      \n","_________________________________________________________________\n","activation_17 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_25 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_16 (MaxPooling (None, 8, 60, 60, 16)     0         \n","_________________________________________________________________\n","conv3d_18 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n","_________________________________________________________________\n","activation_18 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_26 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","conv3d_19 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n","_________________________________________________________________\n","activation_19 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_27 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","max_pooling3d_17 (MaxPooling (None, 4, 30, 30, 32)     0         \n","_________________________________________________________________\n","conv3d_20 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n","_________________________________________________________________\n","activation_20 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_28 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","conv3d_21 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n","_________________________________________________________________\n","activation_21 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_29 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_18 (MaxPooling (None, 2, 15, 15, 64)     0         \n","_________________________________________________________________\n","conv3d_22 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n","_________________________________________________________________\n","activation_22 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_30 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","conv3d_23 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n","_________________________________________________________________\n","activation_23 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_31 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_19 (MaxPooling (None, 1, 7, 7, 128)      0         \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 256)               1605888   \n","_________________________________________________________________\n","batch_normalization_32 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 256)               65792     \n","_________________________________________________________________\n","batch_normalization_33 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 5)                 1285      \n","=================================================================\n","Total params: 2,556,533\n","Trainable params: 2,554,549\n","Non-trainable params: 1,984\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dIO20j-2CCb","executionInfo":{"status":"ok","timestamp":1624044285648,"user_tz":-330,"elapsed":2576519,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"8ec3c91c-972a-4862-81f3-a66537c35f6d"},"source":["print(\"Total Params:\", conv_3d4_model.count_params())\n","history_model4=conv_3d4.train_model(conv_3d4_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 2556533\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/30\n","34/34 [==============================] - 96s 3s/step - loss: 2.1865 - categorical_accuracy: 0.3283 - val_loss: 1.6844 - val_categorical_accuracy: 0.2400\n","\n","Epoch 00001: val_loss improved from inf to 1.68436, saving model to model_init_2021-06-1818_41_49.055826/model-00001-2.04445-0.36275-1.68436-0.24000.h5\n","Epoch 2/30\n","34/34 [==============================] - 84s 3s/step - loss: 1.5520 - categorical_accuracy: 0.4617 - val_loss: 2.3183 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00002: val_loss did not improve from 1.68436\n","Epoch 3/30\n","34/34 [==============================] - 86s 3s/step - loss: 1.3606 - categorical_accuracy: 0.5175 - val_loss: 2.2149 - val_categorical_accuracy: 0.2200\n","\n","Epoch 00003: val_loss did not improve from 1.68436\n","Epoch 4/30\n","34/34 [==============================] - 84s 3s/step - loss: 1.3936 - categorical_accuracy: 0.5163 - val_loss: 1.8414 - val_categorical_accuracy: 0.3100\n","\n","Epoch 00004: val_loss did not improve from 1.68436\n","Epoch 5/30\n","34/34 [==============================] - 85s 3s/step - loss: 1.3558 - categorical_accuracy: 0.4997 - val_loss: 1.8521 - val_categorical_accuracy: 0.3300\n","\n","Epoch 00005: val_loss did not improve from 1.68436\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 6/30\n","34/34 [==============================] - 83s 3s/step - loss: 1.2666 - categorical_accuracy: 0.5496 - val_loss: 2.5700 - val_categorical_accuracy: 0.2000\n","\n","Epoch 00006: val_loss did not improve from 1.68436\n","Epoch 7/30\n","34/34 [==============================] - 86s 3s/step - loss: 1.0261 - categorical_accuracy: 0.6186 - val_loss: 1.9818 - val_categorical_accuracy: 0.3000\n","\n","Epoch 00007: val_loss did not improve from 1.68436\n","Epoch 8/30\n","34/34 [==============================] - 86s 3s/step - loss: 1.0642 - categorical_accuracy: 0.6112 - val_loss: 2.1183 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00008: val_loss did not improve from 1.68436\n","Epoch 9/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.9424 - categorical_accuracy: 0.6219 - val_loss: 2.0540 - val_categorical_accuracy: 0.3500\n","\n","Epoch 00009: val_loss did not improve from 1.68436\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 10/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.8230 - categorical_accuracy: 0.6839 - val_loss: 1.9604 - val_categorical_accuracy: 0.3300\n","\n","Epoch 00010: val_loss did not improve from 1.68436\n","Epoch 11/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.7807 - categorical_accuracy: 0.6958 - val_loss: 1.5466 - val_categorical_accuracy: 0.4100\n","\n","Epoch 00011: val_loss improved from 1.68436 to 1.54662, saving model to model_init_2021-06-1818_41_49.055826/model-00011-0.80983-0.68477-1.54662-0.41000.h5\n","Epoch 12/30\n","34/34 [==============================] - 87s 3s/step - loss: 0.7760 - categorical_accuracy: 0.6987 - val_loss: 1.4426 - val_categorical_accuracy: 0.4200\n","\n","Epoch 00012: val_loss improved from 1.54662 to 1.44259, saving model to model_init_2021-06-1818_41_49.055826/model-00012-0.80982-0.68778-1.44259-0.42000.h5\n","Epoch 13/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.7695 - categorical_accuracy: 0.7073 - val_loss: 1.3508 - val_categorical_accuracy: 0.4900\n","\n","Epoch 00013: val_loss improved from 1.44259 to 1.35082, saving model to model_init_2021-06-1818_41_49.055826/model-00013-0.75373-0.71267-1.35082-0.49000.h5\n","Epoch 14/30\n","34/34 [==============================] - 88s 3s/step - loss: 0.7603 - categorical_accuracy: 0.7142 - val_loss: 1.3273 - val_categorical_accuracy: 0.5400\n","\n","Epoch 00014: val_loss improved from 1.35082 to 1.32726, saving model to model_init_2021-06-1818_41_49.055826/model-00014-0.76726-0.70588-1.32726-0.54000.h5\n","Epoch 15/30\n","34/34 [==============================] - 87s 3s/step - loss: 0.8366 - categorical_accuracy: 0.6909 - val_loss: 1.1022 - val_categorical_accuracy: 0.6300\n","\n","Epoch 00015: val_loss improved from 1.32726 to 1.10222, saving model to model_init_2021-06-1818_41_49.055826/model-00015-0.77793-0.71041-1.10222-0.63000.h5\n","Epoch 16/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.7383 - categorical_accuracy: 0.7116 - val_loss: 1.0044 - val_categorical_accuracy: 0.6800\n","\n","Epoch 00016: val_loss improved from 1.10222 to 1.00442, saving model to model_init_2021-06-1818_41_49.055826/model-00016-0.77641-0.70588-1.00442-0.68000.h5\n","Epoch 17/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.6874 - categorical_accuracy: 0.7298 - val_loss: 0.7891 - val_categorical_accuracy: 0.7300\n","\n","Epoch 00017: val_loss improved from 1.00442 to 0.78915, saving model to model_init_2021-06-1818_41_49.055826/model-00017-0.71049-0.72323-0.78915-0.73000.h5\n","Epoch 18/30\n","34/34 [==============================] - 84s 3s/step - loss: 0.6911 - categorical_accuracy: 0.7386 - val_loss: 0.8323 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00018: val_loss did not improve from 0.78915\n","Epoch 19/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.6918 - categorical_accuracy: 0.7393 - val_loss: 0.8806 - val_categorical_accuracy: 0.7400\n","\n","Epoch 00019: val_loss did not improve from 0.78915\n","Epoch 20/30\n","34/34 [==============================] - 84s 3s/step - loss: 0.7082 - categorical_accuracy: 0.7376 - val_loss: 0.9066 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00020: val_loss did not improve from 0.78915\n","Epoch 21/30\n","34/34 [==============================] - 84s 3s/step - loss: 0.6388 - categorical_accuracy: 0.7497 - val_loss: 0.7749 - val_categorical_accuracy: 0.7400\n","\n","Epoch 00021: val_loss improved from 0.78915 to 0.77487, saving model to model_init_2021-06-1818_41_49.055826/model-00021-0.65097-0.74284-0.77487-0.74000.h5\n","Epoch 22/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.6755 - categorical_accuracy: 0.7412 - val_loss: 0.6982 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00022: val_loss improved from 0.77487 to 0.69824, saving model to model_init_2021-06-1818_41_49.055826/model-00022-0.68785-0.73303-0.69824-0.72000.h5\n","Epoch 23/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.6642 - categorical_accuracy: 0.7253 - val_loss: 0.6446 - val_categorical_accuracy: 0.8000\n","\n","Epoch 00023: val_loss improved from 0.69824 to 0.64457, saving model to model_init_2021-06-1818_41_49.055826/model-00023-0.66807-0.72624-0.64457-0.80000.h5\n","Epoch 24/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.6614 - categorical_accuracy: 0.7642 - val_loss: 0.8850 - val_categorical_accuracy: 0.6600\n","\n","Epoch 00024: val_loss did not improve from 0.64457\n","Epoch 25/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.6280 - categorical_accuracy: 0.7605 - val_loss: 0.6176 - val_categorical_accuracy: 0.7600\n","\n","Epoch 00025: val_loss improved from 0.64457 to 0.61758, saving model to model_init_2021-06-1818_41_49.055826/model-00025-0.64047-0.75264-0.61758-0.76000.h5\n","Epoch 26/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.6706 - categorical_accuracy: 0.7577 - val_loss: 0.7376 - val_categorical_accuracy: 0.7300\n","\n","Epoch 00026: val_loss did not improve from 0.61758\n","Epoch 27/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.6840 - categorical_accuracy: 0.7322 - val_loss: 0.6884 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00027: val_loss did not improve from 0.61758\n","Epoch 28/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.7093 - categorical_accuracy: 0.7488 - val_loss: 0.7437 - val_categorical_accuracy: 0.7300\n","\n","Epoch 00028: val_loss did not improve from 0.61758\n","Epoch 29/30\n","34/34 [==============================] - 86s 3s/step - loss: 0.5701 - categorical_accuracy: 0.7927 - val_loss: 0.7006 - val_categorical_accuracy: 0.6900\n","\n","Epoch 00029: val_loss did not improve from 0.61758\n","\n","Epoch 00029: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n","Epoch 30/30\n","34/34 [==============================] - 85s 3s/step - loss: 0.5244 - categorical_accuracy: 0.7931 - val_loss: 0.7370 - val_categorical_accuracy: 0.7300\n","\n","Epoch 00030: val_loss did not improve from 0.61758\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"prHMZ9v66G7j"},"source":["### **Model 5 Adding dropout at convolution layers**"]},{"cell_type":"code","metadata":{"id":"C4QHO3e_2CnY"},"source":["class ModelConv3D5(ModelBuilder):\n","    \n","    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(16, filtersize, padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        model.add(Dropout(dropout))\n","\n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(32, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        model.add(Dropout(dropout))\n","\n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(64, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        model.add(Dropout(dropout))\n","\n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(Conv3D(128, filtersize, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        \n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        model.add(Dropout(dropout))\n","        \n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam()\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4jBcBXK6R3K","executionInfo":{"status":"ok","timestamp":1624044465362,"user_tz":-330,"elapsed":486,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"8da30e2c-47e0-421c-89af-dd809de77f62"},"source":["conv_3d5=ModelConv3D5()\n","conv_3d5.initialize_path(project_folder)\n","conv_3d5.initialize_image_properties(image_height=120,image_width=120)\n","conv_3d5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n","conv_3d5_model=conv_3d5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n","conv_3d5_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_24 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n","_________________________________________________________________\n","activation_24 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_34 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","conv3d_25 (Conv3D)           (None, 16, 120, 120, 16)  6928      \n","_________________________________________________________________\n","activation_25 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_35 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_20 (MaxPooling (None, 8, 60, 60, 16)     0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 8, 60, 60, 16)     0         \n","_________________________________________________________________\n","conv3d_26 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n","_________________________________________________________________\n","activation_26 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_36 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","conv3d_27 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n","_________________________________________________________________\n","activation_27 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_37 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","max_pooling3d_21 (MaxPooling (None, 4, 30, 30, 32)     0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 4, 30, 30, 32)     0         \n","_________________________________________________________________\n","conv3d_28 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n","_________________________________________________________________\n","activation_28 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_38 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","conv3d_29 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n","_________________________________________________________________\n","activation_29 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_39 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_22 (MaxPooling (None, 2, 15, 15, 64)     0         \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 2, 15, 15, 64)     0         \n","_________________________________________________________________\n","conv3d_30 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n","_________________________________________________________________\n","activation_30 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_40 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","conv3d_31 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n","_________________________________________________________________\n","activation_31 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_41 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_23 (MaxPooling (None, 1, 7, 7, 128)      0         \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 1, 7, 7, 128)      0         \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 256)               1605888   \n","_________________________________________________________________\n","batch_normalization_42 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 256)               65792     \n","_________________________________________________________________\n","batch_normalization_43 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dropout_15 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 5)                 1285      \n","=================================================================\n","Total params: 2,556,533\n","Trainable params: 2,554,549\n","Non-trainable params: 1,984\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8R-mDmjc6US3","executionInfo":{"status":"ok","timestamp":1624045438744,"user_tz":-330,"elapsed":930659,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"f470dc4a-2855-42b1-f7dc-9aa37c688172"},"source":["print(\"Total Params:\", conv_3d5_model.count_params())\n","history_model5=conv_3d5.train_model(conv_3d5_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 2556533\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/22\n","34/34 [==============================] - 93s 3s/step - loss: 1.9215 - categorical_accuracy: 0.3227 - val_loss: 2.9831 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00001: val_loss improved from inf to 2.98306, saving model to model_init_2021-06-1819_28_28.283231/model-00001-1.72125-0.37783-2.98306-0.16000.h5\n","Epoch 2/22\n","34/34 [==============================] - 83s 3s/step - loss: 1.3231 - categorical_accuracy: 0.5111 - val_loss: 4.2550 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00002: val_loss did not improve from 2.98306\n","Epoch 3/22\n","34/34 [==============================] - 84s 3s/step - loss: 1.1342 - categorical_accuracy: 0.5635 - val_loss: 7.6320 - val_categorical_accuracy: 0.1300\n","\n","Epoch 00003: val_loss did not improve from 2.98306\n","Epoch 4/22\n","34/34 [==============================] - 84s 3s/step - loss: 0.9984 - categorical_accuracy: 0.6267 - val_loss: 4.3512 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00004: val_loss did not improve from 2.98306\n","Epoch 5/22\n","34/34 [==============================] - 83s 3s/step - loss: 0.9430 - categorical_accuracy: 0.6234 - val_loss: 4.7279 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00005: val_loss did not improve from 2.98306\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 6/22\n","34/34 [==============================] - 83s 2s/step - loss: 0.7377 - categorical_accuracy: 0.7217 - val_loss: 5.1253 - val_categorical_accuracy: 0.2200\n","\n","Epoch 00006: val_loss did not improve from 2.98306\n","Epoch 7/22\n","34/34 [==============================] - 83s 2s/step - loss: 0.6699 - categorical_accuracy: 0.7397 - val_loss: 5.7843 - val_categorical_accuracy: 0.2000\n","\n","Epoch 00007: val_loss did not improve from 2.98306\n","Epoch 8/22\n","34/34 [==============================] - 83s 3s/step - loss: 0.6491 - categorical_accuracy: 0.7746 - val_loss: 5.8296 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00008: val_loss did not improve from 2.98306\n","Epoch 9/22\n","34/34 [==============================] - 83s 3s/step - loss: 0.5208 - categorical_accuracy: 0.8157 - val_loss: 5.3821 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00009: val_loss did not improve from 2.98306\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 10/22\n","34/34 [==============================] - 83s 3s/step - loss: 0.5427 - categorical_accuracy: 0.7979 - val_loss: 5.3127 - val_categorical_accuracy: 0.2000\n","\n","Epoch 00010: val_loss did not improve from 2.98306\n","Epoch 11/22\n","34/34 [==============================] - 86s 3s/step - loss: 0.4292 - categorical_accuracy: 0.8435 - val_loss: 4.7679 - val_categorical_accuracy: 0.2800\n","\n","Epoch 00011: val_loss did not improve from 2.98306\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Roz04Izl97rE"},"source":["### **Model 6 - reducing the number of parameters**"]},{"cell_type":"code","metadata":{"id":"XsDDkvFO-B3_"},"source":["class ModelConv3D6(ModelBuilder):\n","    \n","    def define_model(self,dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, (3, 3, 3), padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        \n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam(lr=0.0002)\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxbIUQv4-S85","executionInfo":{"status":"ok","timestamp":1624045647275,"user_tz":-330,"elapsed":519,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"aab5359a-9f11-4aae-cf46-74a72732f4df"},"source":["conv_3d6=ModelConv3D6()\n","conv_3d6.initialize_path(project_folder)\n","conv_3d6.initialize_image_properties(image_height=100,image_width=100)\n","conv_3d6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n","conv_3d6_model=conv_3d6.define_model(dense_neurons=128,dropout=0.25)\n","conv_3d6_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_32 (Conv3D)           (None, 16, 100, 100, 16)  1312      \n","_________________________________________________________________\n","activation_32 (Activation)   (None, 16, 100, 100, 16)  0         \n","_________________________________________________________________\n","batch_normalization_44 (Batc (None, 16, 100, 100, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_24 (MaxPooling (None, 8, 50, 50, 16)     0         \n","_________________________________________________________________\n","conv3d_33 (Conv3D)           (None, 8, 50, 50, 32)     4128      \n","_________________________________________________________________\n","activation_33 (Activation)   (None, 8, 50, 50, 32)     0         \n","_________________________________________________________________\n","batch_normalization_45 (Batc (None, 8, 50, 50, 32)     128       \n","_________________________________________________________________\n","max_pooling3d_25 (MaxPooling (None, 4, 25, 25, 32)     0         \n","_________________________________________________________________\n","conv3d_34 (Conv3D)           (None, 4, 25, 25, 64)     16448     \n","_________________________________________________________________\n","activation_34 (Activation)   (None, 4, 25, 25, 64)     0         \n","_________________________________________________________________\n","batch_normalization_46 (Batc (None, 4, 25, 25, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_26 (MaxPooling (None, 2, 12, 12, 64)     0         \n","_________________________________________________________________\n","conv3d_35 (Conv3D)           (None, 2, 12, 12, 128)    65664     \n","_________________________________________________________________\n","activation_35 (Activation)   (None, 2, 12, 12, 128)    0         \n","_________________________________________________________________\n","batch_normalization_47 (Batc (None, 2, 12, 12, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_27 (MaxPooling (None, 1, 6, 6, 128)      0         \n","_________________________________________________________________\n","flatten_6 (Flatten)          (None, 4608)              0         \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 128)               589952    \n","_________________________________________________________________\n","batch_normalization_48 (Batc (None, 128)               512       \n","_________________________________________________________________\n","dropout_16 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 128)               16512     \n","_________________________________________________________________\n","batch_normalization_49 (Batc (None, 128)               512       \n","_________________________________________________________________\n","dropout_17 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 5)                 645       \n","=================================================================\n","Total params: 696,645\n","Trainable params: 695,653\n","Non-trainable params: 992\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jH2L_tXX-Tkd","executionInfo":{"status":"ok","timestamp":1624046545643,"user_tz":-330,"elapsed":896618,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"50b554dc-5d21-4a48-f248-2a8c9d0780b9"},"source":["print(\"Total Params:\", conv_3d6_model.count_params())\n","history_model6=conv_3d6.train_model(conv_3d6_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 696645\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/30\n","34/34 [==============================] - 86s 3s/step - loss: 2.0738 - categorical_accuracy: 0.2855 - val_loss: 1.7574 - val_categorical_accuracy: 0.2400\n","\n","Epoch 00001: val_loss improved from inf to 1.75744, saving model to model_init_2021-06-1819_47_28.873137/model-00001-1.83050-0.36501-1.75744-0.24000.h5\n","Epoch 2/30\n","34/34 [==============================] - 81s 2s/step - loss: 1.1989 - categorical_accuracy: 0.5362 - val_loss: 2.5504 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00002: val_loss did not improve from 1.75744\n","Epoch 3/30\n","34/34 [==============================] - 80s 2s/step - loss: 1.0000 - categorical_accuracy: 0.6212 - val_loss: 4.1543 - val_categorical_accuracy: 0.1200\n","\n","Epoch 00003: val_loss did not improve from 1.75744\n","Epoch 4/30\n","34/34 [==============================] - 81s 2s/step - loss: 0.7933 - categorical_accuracy: 0.6846 - val_loss: 5.3436 - val_categorical_accuracy: 0.1500\n","\n","Epoch 00004: val_loss did not improve from 1.75744\n","Epoch 5/30\n","34/34 [==============================] - 80s 2s/step - loss: 0.6808 - categorical_accuracy: 0.7432 - val_loss: 5.4965 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00005: val_loss did not improve from 1.75744\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n","Epoch 6/30\n","34/34 [==============================] - 81s 2s/step - loss: 0.6036 - categorical_accuracy: 0.7736 - val_loss: 6.1769 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00006: val_loss did not improve from 1.75744\n","Epoch 7/30\n","34/34 [==============================] - 81s 2s/step - loss: 0.5749 - categorical_accuracy: 0.7981 - val_loss: 6.5512 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00007: val_loss did not improve from 1.75744\n","Epoch 8/30\n","34/34 [==============================] - 80s 2s/step - loss: 0.5506 - categorical_accuracy: 0.7952 - val_loss: 5.6131 - val_categorical_accuracy: 0.2000\n","\n","Epoch 00008: val_loss did not improve from 1.75744\n","Epoch 9/30\n","34/34 [==============================] - 82s 2s/step - loss: 0.5777 - categorical_accuracy: 0.7852 - val_loss: 6.4358 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00009: val_loss did not improve from 1.75744\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n","Epoch 10/30\n","34/34 [==============================] - 81s 2s/step - loss: 0.5164 - categorical_accuracy: 0.8134 - val_loss: 5.3379 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00010: val_loss did not improve from 1.75744\n","Epoch 11/30\n","34/34 [==============================] - 82s 2s/step - loss: 0.5783 - categorical_accuracy: 0.7774 - val_loss: 4.8243 - val_categorical_accuracy: 0.2600\n","\n","Epoch 00011: val_loss did not improve from 1.75744\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EsVKBhYTCYLW"},"source":["**Model 7 - reducing the number of parameters**"]},{"cell_type":"code","metadata":{"id":"IUMKDumL-Xiu"},"source":["class ModelConv3D7(ModelBuilder):\n","    \n","    def define_model(self,dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","        model.add(Conv3D(16, (3, 3, 3), padding='same',\n","                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        \n","        model.add(Flatten())\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(dropout))\n","\n","        model.add(Dense(self.num_classes,activation='softmax'))\n","\n","        optimiser = optimizers.Adam(lr=0.0002)\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t545Q8F5CgST","executionInfo":{"status":"ok","timestamp":1624046575411,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"42fb6e9f-861b-490c-f28b-09d1c9c4cb13"},"source":["conv_3d7=ModelConv3D7()\n","conv_3d7.initialize_path(project_folder)\n","conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n","conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n","conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n","conv_3d7_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv3d_36 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n","_________________________________________________________________\n","activation_36 (Activation)   (None, 16, 120, 120, 16)  0         \n","_________________________________________________________________\n","batch_normalization_50 (Batc (None, 16, 120, 120, 16)  64        \n","_________________________________________________________________\n","max_pooling3d_28 (MaxPooling (None, 8, 60, 60, 16)     0         \n","_________________________________________________________________\n","conv3d_37 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n","_________________________________________________________________\n","activation_37 (Activation)   (None, 8, 60, 60, 32)     0         \n","_________________________________________________________________\n","batch_normalization_51 (Batc (None, 8, 60, 60, 32)     128       \n","_________________________________________________________________\n","max_pooling3d_29 (MaxPooling (None, 4, 30, 30, 32)     0         \n","_________________________________________________________________\n","conv3d_38 (Conv3D)           (None, 4, 30, 30, 64)     16448     \n","_________________________________________________________________\n","activation_38 (Activation)   (None, 4, 30, 30, 64)     0         \n","_________________________________________________________________\n","batch_normalization_52 (Batc (None, 4, 30, 30, 64)     256       \n","_________________________________________________________________\n","max_pooling3d_30 (MaxPooling (None, 2, 15, 15, 64)     0         \n","_________________________________________________________________\n","conv3d_39 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n","_________________________________________________________________\n","activation_39 (Activation)   (None, 2, 15, 15, 128)    0         \n","_________________________________________________________________\n","batch_normalization_53 (Batc (None, 2, 15, 15, 128)    512       \n","_________________________________________________________________\n","max_pooling3d_31 (MaxPooling (None, 1, 7, 7, 128)      0         \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_21 (Dense)             (None, 64)                401472    \n","_________________________________________________________________\n","batch_normalization_54 (Batc (None, 64)                256       \n","_________________________________________________________________\n","dropout_18 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_22 (Dense)             (None, 64)                4160      \n","_________________________________________________________________\n","batch_normalization_55 (Batc (None, 64)                256       \n","_________________________________________________________________\n","dropout_19 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_23 (Dense)             (None, 5)                 325       \n","=================================================================\n","Total params: 504,709\n","Trainable params: 503,973\n","Non-trainable params: 736\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vom7XyBVCjX4","executionInfo":{"status":"ok","timestamp":1624047520903,"user_tz":-330,"elapsed":115127,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"fea6099f-9788-4af7-93e4-1a0841a6dce2"},"source":["print(\"Total Params:\", conv_3d7_model.count_params())\n","history_model7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 504709\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/25\n","34/34 [==============================] - 91s 3s/step - loss: 2.0444 - categorical_accuracy: 0.3002 - val_loss: 1.9174 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00001: val_loss improved from inf to 1.91735, saving model to model_init_2021-06-1820_02_55.390055/model-00001-1.82015-0.36199-1.91735-0.21000.h5\n","Epoch 2/25\n","34/34 [==============================] - 92s 3s/step - loss: 1.3861 - categorical_accuracy: 0.4725 - val_loss: 2.4035 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00002: val_loss did not improve from 1.91735\n","Epoch 3/25\n","34/34 [==============================] - 89s 3s/step - loss: 1.1070 - categorical_accuracy: 0.5488 - val_loss: 2.6107 - val_categorical_accuracy: 0.1800\n","\n","Epoch 00003: val_loss did not improve from 1.91735\n","Epoch 4/25\n","34/34 [==============================] - 83s 3s/step - loss: 0.9647 - categorical_accuracy: 0.6397 - val_loss: 3.3009 - val_categorical_accuracy: 0.1400\n","\n","Epoch 00004: val_loss did not improve from 1.91735\n","Epoch 5/25\n","34/34 [==============================] - 84s 3s/step - loss: 0.9056 - categorical_accuracy: 0.6462 - val_loss: 3.6718 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00005: val_loss did not improve from 1.91735\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n","Epoch 6/25\n","34/34 [==============================] - 86s 3s/step - loss: 0.8055 - categorical_accuracy: 0.6866 - val_loss: 4.1161 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00006: val_loss did not improve from 1.91735\n","Epoch 7/25\n","34/34 [==============================] - 83s 3s/step - loss: 0.8205 - categorical_accuracy: 0.7011 - val_loss: 3.6943 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00007: val_loss did not improve from 1.91735\n","Epoch 8/25\n","34/34 [==============================] - 84s 3s/step - loss: 0.6682 - categorical_accuracy: 0.7447 - val_loss: 4.4443 - val_categorical_accuracy: 0.1500\n","\n","Epoch 00008: val_loss did not improve from 1.91735\n","Epoch 9/25\n","34/34 [==============================] - 82s 2s/step - loss: 0.6689 - categorical_accuracy: 0.7273 - val_loss: 3.9430 - val_categorical_accuracy: 0.2400\n","\n","Epoch 00009: val_loss did not improve from 1.91735\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n","Epoch 10/25\n","34/34 [==============================] - 83s 3s/step - loss: 0.6853 - categorical_accuracy: 0.7551 - val_loss: 4.2241 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00010: val_loss did not improve from 1.91735\n","Epoch 11/25\n","34/34 [==============================] - 85s 3s/step - loss: 0.6830 - categorical_accuracy: 0.7450 - val_loss: 3.8605 - val_categorical_accuracy: 0.3200\n","\n","Epoch 00011: val_loss did not improve from 1.91735\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ihxkVkvTGyq2"},"source":["### **Model 8 - CNN- LSTM Model**"]},{"cell_type":"code","metadata":{"id":"HF5mX_YECnqx"},"source":["class RNNCNN1(ModelBuilder):\n","    \n","    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","\n","        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n","                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        #model.add(TimeDistributed(Conv2D(512, (2, 2) , padding='valid', activation='relu')))\n","       # model.add(TimeDistributed(BatchNormalization()))\n","       # model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","        model.add(TimeDistributed(Flatten()))\n","\n","\n","        model.add(LSTM(lstm_cells))\n","        model.add(Dropout(dropout))\n","        \n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(Dropout(dropout))\n","        \n","        model.add(Dense(self.num_classes, activation='softmax'))\n","        optimiser = optimizers.Adam()\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NArXbCkHCuR","executionInfo":{"status":"ok","timestamp":1624047539306,"user_tz":-330,"elapsed":1212,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"0a44a116-00a3-46ef-e1a0-f9e3dc86faf6"},"source":["rnn_cnn1=RNNCNN1()\n","rnn_cnn1.initialize_path(project_folder)\n","rnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\n","rnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n","rnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n","rnn_cnn1_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","time_distributed (TimeDistri (None, 18, 120, 120, 16)  448       \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 18, 120, 120, 16)  64        \n","_________________________________________________________________\n","time_distributed_2 (TimeDist (None, 18, 60, 60, 16)    0         \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 18, 60, 60, 32)    4640      \n","_________________________________________________________________\n","time_distributed_4 (TimeDist (None, 18, 60, 60, 32)    128       \n","_________________________________________________________________\n","time_distributed_5 (TimeDist (None, 18, 30, 30, 32)    0         \n","_________________________________________________________________\n","time_distributed_6 (TimeDist (None, 18, 30, 30, 64)    18496     \n","_________________________________________________________________\n","time_distributed_7 (TimeDist (None, 18, 30, 30, 64)    256       \n","_________________________________________________________________\n","time_distributed_8 (TimeDist (None, 18, 15, 15, 64)    0         \n","_________________________________________________________________\n","time_distributed_9 (TimeDist (None, 18, 15, 15, 128)   73856     \n","_________________________________________________________________\n","time_distributed_10 (TimeDis (None, 18, 15, 15, 128)   512       \n","_________________________________________________________________\n","time_distributed_11 (TimeDis (None, 18, 7, 7, 128)     0         \n","_________________________________________________________________\n","time_distributed_12 (TimeDis (None, 18, 7, 7, 256)     295168    \n","_________________________________________________________________\n","time_distributed_13 (TimeDis (None, 18, 7, 7, 256)     1024      \n","_________________________________________________________________\n","time_distributed_14 (TimeDis (None, 18, 3, 3, 256)     0         \n","_________________________________________________________________\n","time_distributed_15 (TimeDis (None, 18, 2304)          0         \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 128)               1245696   \n","_________________________________________________________________\n","dropout_20 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_24 (Dense)             (None, 128)               16512     \n","_________________________________________________________________\n","dropout_21 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_25 (Dense)             (None, 5)                 645       \n","=================================================================\n","Total params: 1,657,445\n","Trainable params: 1,656,453\n","Non-trainable params: 992\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tz-00YHUHG3c","executionInfo":{"status":"ok","timestamp":1624048755135,"user_tz":-330,"elapsed":1215845,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"74690024-4204-4218-951c-2016d0bd1c4e"},"source":["print(\"Total Params:\", rnn_cnn1_model.count_params())\n","history_model9=rnn_cnn1.train_model(rnn_cnn1_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 1657445\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/20\n","34/34 [==============================] - 123s 3s/step - loss: 1.5463 - categorical_accuracy: 0.3285 - val_loss: 1.6013 - val_categorical_accuracy: 0.2300\n","\n","Epoch 00001: val_loss improved from inf to 1.60129, saving model to model_init_2021-06-1820_18_57.781018/model-00001-1.43358-0.39367-1.60129-0.23000.h5\n","Epoch 2/20\n","34/34 [==============================] - 106s 3s/step - loss: 1.1191 - categorical_accuracy: 0.5745 - val_loss: 1.7203 - val_categorical_accuracy: 0.2400\n","\n","Epoch 00002: val_loss did not improve from 1.60129\n","Epoch 3/20\n","34/34 [==============================] - 106s 3s/step - loss: 0.9462 - categorical_accuracy: 0.6411 - val_loss: 2.4070 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00003: val_loss did not improve from 1.60129\n","Epoch 4/20\n","34/34 [==============================] - 107s 3s/step - loss: 0.8328 - categorical_accuracy: 0.6745 - val_loss: 2.8168 - val_categorical_accuracy: 0.1400\n","\n","Epoch 00004: val_loss did not improve from 1.60129\n","Epoch 5/20\n","34/34 [==============================] - 109s 3s/step - loss: 0.6985 - categorical_accuracy: 0.7265 - val_loss: 2.7931 - val_categorical_accuracy: 0.2100\n","\n","Epoch 00005: val_loss did not improve from 1.60129\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 6/20\n","34/34 [==============================] - 109s 3s/step - loss: 0.5833 - categorical_accuracy: 0.7715 - val_loss: 2.8916 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00006: val_loss did not improve from 1.60129\n","Epoch 7/20\n","34/34 [==============================] - 107s 3s/step - loss: 0.4493 - categorical_accuracy: 0.8410 - val_loss: 2.4052 - val_categorical_accuracy: 0.1700\n","\n","Epoch 00007: val_loss did not improve from 1.60129\n","Epoch 8/20\n","34/34 [==============================] - 119s 4s/step - loss: 0.3686 - categorical_accuracy: 0.8702 - val_loss: 2.2862 - val_categorical_accuracy: 0.2600\n","\n","Epoch 00008: val_loss did not improve from 1.60129\n","Epoch 9/20\n","34/34 [==============================] - 107s 3s/step - loss: 0.3391 - categorical_accuracy: 0.8938 - val_loss: 2.2302 - val_categorical_accuracy: 0.1400\n","\n","Epoch 00009: val_loss did not improve from 1.60129\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 10/20\n","34/34 [==============================] - 108s 3s/step - loss: 0.3112 - categorical_accuracy: 0.8827 - val_loss: 1.8149 - val_categorical_accuracy: 0.3400\n","\n","Epoch 00010: val_loss did not improve from 1.60129\n","Epoch 11/20\n","34/34 [==============================] - 110s 3s/step - loss: 0.3038 - categorical_accuracy: 0.9087 - val_loss: 1.7002 - val_categorical_accuracy: 0.4500\n","\n","Epoch 00011: val_loss did not improve from 1.60129\n","Epoch 00011: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NgQ_ePCONaaA"},"source":["### **More Augmentation**"]},{"cell_type":"code","metadata":{"id":"jQwl8BfUNitn"},"source":["class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n","    \n","    def initialize_path(self,project_folder):\n","        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n","        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n","        self.train_path = project_folder + '/' + 'train'\n","        self.val_path =  project_folder + '/' + 'val'\n","        self.num_train_sequences = len(self.train_doc)\n","        self.num_val_sequences = len(self.val_doc)\n","        \n","    def initialize_image_properties(self,image_height=100,image_width=100):\n","        self.image_height=image_height\n","        self.image_width=image_width\n","        self.channels=3\n","        self.num_classes=5\n","        self.total_frames=30\n","          \n","    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n","        self.frames_to_sample=frames_to_sample\n","        self.batch_size=batch_size\n","        self.num_epochs=num_epochs\n","        \n","        \n","    def generator(self,source_path, folder_list, augment=False):\n","        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n","        batch_size=self.batch_size\n","        while True:\n","            t = np.random.permutation(folder_list)\n","            num_batches = len(t)//batch_size\n","        \n","            for batch in range(num_batches): \n","                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n","                yield batch_data, batch_labels \n","\n","            remaining_seq=len(t)%batch_size\n","        \n","            if (remaining_seq != 0):\n","                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n","                yield batch_data, batch_labels \n","    \n","    \n","    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n","    \n","        seq_len = remaining_seq if remaining_seq else batch_size\n","    \n","        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n","        batch_labels = np.zeros((seq_len,self.num_classes)) \n","    \n","        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n","\n","        \n","        for folder in range(seq_len): \n","            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n","            for idx,item in enumerate(img_idx): \n","                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                image_resized=imresize(image,(self.image_height,self.image_width,3))\n","            \n","\n","                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n","                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n","                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n","            \n","                if (augment):\n","                    shifted = cv2.warpAffine(image, \n","                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n","                                            (image.shape[1], image.shape[0]))\n","                    \n","                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n","\n","                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n","                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n","                    \n","                    cropped=shifted[x0:x1,y0:y1,:]\n","                    \n","                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n","                    \n","                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n","                                                np.random.randint(-10,10), 1.0)\n","                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n","                    \n","                    #shifted = cv2.warpAffine(image_resized, \n","                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n","                    #                        (image_resized.shape[1], image_resized.shape[0]))\n","            \n","                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n","                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n","                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n","                \n","            \n","            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            \n","    \n","        if (augment):\n","            batch_data=np.concatenate([batch_data,batch_data_aug])\n","            batch_labels=np.concatenate([batch_labels,batch_labels])\n","\n","        \n","        return(batch_data,batch_labels)\n","    \n","    \n","    def train_model(self, model, augment_data=False):\n","        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n","        val_generator = self.generator(self.val_path, self.val_doc)\n","\n","        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n","    \n","        if not os.path.exists(model_name):\n","            os.mkdir(model_name)\n","        \n","        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n","        callbacks_list = [checkpoint, LR]\n","\n","        if (self.num_train_sequences%self.batch_size) == 0:\n","            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n","        else:\n","            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n","\n","        if (self.num_val_sequences%self.batch_size) == 0:\n","            validation_steps = int(self.num_val_sequences/self.batch_size)\n","        else:\n","            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n","    \n","        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n","                            callbacks=callbacks_list, validation_data=val_generator, \n","                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n","        return history\n","\n","        \n","    @abc.abstractmethod\n","    def define_model(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRjfLzR3MkFr"},"source":["### **Model 9- CNN LSTM with GRU**"]},{"cell_type":"code","metadata":{"id":"vm-nG6TPHLOf"},"source":["class RNNCNN2(ModelBuilderMoreAugmentation):\n","    \n","    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n","\n","        model = Sequential()\n","\n","        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n","                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","\n","        model.add(TimeDistributed(Flatten()))\n","\n","\n","        model.add(GRU(lstm_cells))\n","        model.add(Dropout(dropout))\n","        \n","        model.add(Dense(dense_neurons,activation='relu'))\n","        model.add(Dropout(dropout))\n","        \n","        model.add(Dense(self.num_classes, activation='softmax'))\n","        optimiser = optimizers.Adam(lr=0.0002)\n","        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6VQ5VOrM4Px","executionInfo":{"status":"ok","timestamp":1624048953965,"user_tz":-330,"elapsed":1094,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"c11dcbc8-e5ad-4961-b113-f999a0ee90be"},"source":["rnn_cnn2=RNNCNN2()\n","rnn_cnn2.initialize_path(project_folder)\n","rnn_cnn2.initialize_image_properties(image_height=120,image_width=120)\n","rnn_cnn2.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n","rnn_cnn2_model=rnn_cnn2.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n","rnn_cnn2_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","time_distributed_16 (TimeDis (None, 18, 120, 120, 16)  448       \n","_________________________________________________________________\n","time_distributed_17 (TimeDis (None, 18, 120, 120, 16)  64        \n","_________________________________________________________________\n","time_distributed_18 (TimeDis (None, 18, 60, 60, 16)    0         \n","_________________________________________________________________\n","time_distributed_19 (TimeDis (None, 18, 60, 60, 32)    4640      \n","_________________________________________________________________\n","time_distributed_20 (TimeDis (None, 18, 60, 60, 32)    128       \n","_________________________________________________________________\n","time_distributed_21 (TimeDis (None, 18, 30, 30, 32)    0         \n","_________________________________________________________________\n","time_distributed_22 (TimeDis (None, 18, 30, 30, 64)    18496     \n","_________________________________________________________________\n","time_distributed_23 (TimeDis (None, 18, 30, 30, 64)    256       \n","_________________________________________________________________\n","time_distributed_24 (TimeDis (None, 18, 15, 15, 64)    0         \n","_________________________________________________________________\n","time_distributed_25 (TimeDis (None, 18, 15, 15, 128)   73856     \n","_________________________________________________________________\n","time_distributed_26 (TimeDis (None, 18, 15, 15, 128)   512       \n","_________________________________________________________________\n","time_distributed_27 (TimeDis (None, 18, 7, 7, 128)     0         \n","_________________________________________________________________\n","time_distributed_28 (TimeDis (None, 18, 6272)          0         \n","_________________________________________________________________\n","gru (GRU)                    (None, 128)               2458368   \n","_________________________________________________________________\n","dropout_22 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 128)               16512     \n","_________________________________________________________________\n","dropout_23 (Dropout)         (None, 128)               0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 5)                 645       \n","=================================================================\n","Total params: 2,573,925\n","Trainable params: 2,573,445\n","Non-trainable params: 480\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76i2q_p2NzBO","executionInfo":{"status":"ok","timestamp":1624051328712,"user_tz":-330,"elapsed":2373179,"user":{"displayName":"Anil Naidu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAqJgSVsKxnk1RnU8d3p1eJ03yNdIhogU92ivk=s64","userId":"15240233656746913556"}},"outputId":"f8a72d97-8ac0-4da5-f4b5-ea94e22db73a"},"source":["print(\"Total Params:\", rnn_cnn2_model.count_params())\n","history_model17=rnn_cnn2.train_model(rnn_cnn2_model,augment_data=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Params: 2573925\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/20\n","34/34 [==============================] - 132s 4s/step - loss: 1.6307 - categorical_accuracy: 0.2728 - val_loss: 1.7418 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00001: saving model to model_init_2021-06-1820_42_34.362893/model-00001-1.49353-0.34540-1.74180-0.16000.h5\n","Epoch 2/20\n","34/34 [==============================] - 122s 4s/step - loss: 1.0948 - categorical_accuracy: 0.5392 - val_loss: 2.3823 - val_categorical_accuracy: 0.1200\n","\n","Epoch 00002: saving model to model_init_2021-06-1820_42_34.362893/model-00002-1.05782-0.56335-2.38226-0.12000.h5\n","Epoch 3/20\n","34/34 [==============================] - 114s 3s/step - loss: 0.9229 - categorical_accuracy: 0.6518 - val_loss: 2.7761 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00003: saving model to model_init_2021-06-1820_42_34.362893/model-00003-0.89834-0.67044-2.77609-0.19000.h5\n","Epoch 4/20\n","34/34 [==============================] - 115s 3s/step - loss: 0.7350 - categorical_accuracy: 0.7315 - val_loss: 2.8968 - val_categorical_accuracy: 0.1900\n","\n","Epoch 00004: saving model to model_init_2021-06-1820_42_34.362893/model-00004-0.72617-0.73379-2.89679-0.19000.h5\n","Epoch 5/20\n","34/34 [==============================] - 122s 4s/step - loss: 0.6208 - categorical_accuracy: 0.7818 - val_loss: 3.1068 - val_categorical_accuracy: 0.1600\n","\n","Epoch 00005: saving model to model_init_2021-06-1820_42_34.362893/model-00005-0.60554-0.79110-3.10682-0.16000.h5\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n","Epoch 6/20\n","34/34 [==============================] - 120s 4s/step - loss: 0.5027 - categorical_accuracy: 0.8251 - val_loss: 3.0831 - val_categorical_accuracy: 0.2500\n","\n","Epoch 00006: saving model to model_init_2021-06-1820_42_34.362893/model-00006-0.47990-0.83560-3.08308-0.25000.h5\n","Epoch 7/20\n","34/34 [==============================] - 112s 3s/step - loss: 0.4281 - categorical_accuracy: 0.8692 - val_loss: 3.0366 - val_categorical_accuracy: 0.2800\n","\n","Epoch 00007: saving model to model_init_2021-06-1820_42_34.362893/model-00007-0.42584-0.86953-3.03657-0.28000.h5\n","Epoch 8/20\n","34/34 [==============================] - 112s 3s/step - loss: 0.4173 - categorical_accuracy: 0.8794 - val_loss: 3.1904 - val_categorical_accuracy: 0.2200\n","\n","Epoch 00008: saving model to model_init_2021-06-1820_42_34.362893/model-00008-0.38964-0.88989-3.19036-0.22000.h5\n","Epoch 9/20\n","34/34 [==============================] - 113s 3s/step - loss: 0.3550 - categorical_accuracy: 0.8960 - val_loss: 2.9733 - val_categorical_accuracy: 0.2800\n","\n","Epoch 00009: saving model to model_init_2021-06-1820_42_34.362893/model-00009-0.35496-0.89744-2.97326-0.28000.h5\n","\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n","Epoch 10/20\n","34/34 [==============================] - 122s 4s/step - loss: 0.3402 - categorical_accuracy: 0.8995 - val_loss: 2.7033 - val_categorical_accuracy: 0.3600\n","\n","Epoch 00010: saving model to model_init_2021-06-1820_42_34.362893/model-00010-0.33580-0.90196-2.70327-0.36000.h5\n","Epoch 11/20\n","34/34 [==============================] - 129s 4s/step - loss: 0.3637 - categorical_accuracy: 0.8917 - val_loss: 2.7898 - val_categorical_accuracy: 0.2900\n","\n","Epoch 00011: saving model to model_init_2021-06-1820_42_34.362893/model-00011-0.34119-0.89819-2.78982-0.29000.h5\n","Epoch 12/20\n","34/34 [==============================] - 114s 3s/step - loss: 0.2987 - categorical_accuracy: 0.9241 - val_loss: 2.6409 - val_categorical_accuracy: 0.3500\n","\n","Epoch 00012: saving model to model_init_2021-06-1820_42_34.362893/model-00012-0.30576-0.92157-2.64089-0.35000.h5\n","Epoch 13/20\n","34/34 [==============================] - 116s 4s/step - loss: 0.3252 - categorical_accuracy: 0.9206 - val_loss: 2.2367 - val_categorical_accuracy: 0.3700\n","\n","Epoch 00013: saving model to model_init_2021-06-1820_42_34.362893/model-00013-0.30978-0.92232-2.23672-0.37000.h5\n","\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5999999959603884e-06.\n","Epoch 14/20\n","34/34 [==============================] - 122s 4s/step - loss: 0.2961 - categorical_accuracy: 0.9225 - val_loss: 1.8943 - val_categorical_accuracy: 0.4600\n","\n","Epoch 00014: saving model to model_init_2021-06-1820_42_34.362893/model-00014-0.30242-0.92232-1.89434-0.46000.h5\n","Epoch 15/20\n","34/34 [==============================] - 117s 4s/step - loss: 0.3216 - categorical_accuracy: 0.9233 - val_loss: 1.8361 - val_categorical_accuracy: 0.4600\n","\n","Epoch 00015: saving model to model_init_2021-06-1820_42_34.362893/model-00015-0.30853-0.92383-1.83608-0.46000.h5\n","Epoch 16/20\n","34/34 [==============================] - 116s 4s/step - loss: 0.3119 - categorical_accuracy: 0.9243 - val_loss: 1.3881 - val_categorical_accuracy: 0.5500\n","\n","Epoch 00016: saving model to model_init_2021-06-1820_42_34.362893/model-00016-0.31000-0.92157-1.38809-0.55000.h5\n","Epoch 17/20\n","34/34 [==============================] - 116s 4s/step - loss: 0.2986 - categorical_accuracy: 0.9238 - val_loss: 1.2421 - val_categorical_accuracy: 0.6100\n","\n","Epoch 00017: saving model to model_init_2021-06-1820_42_34.362893/model-00017-0.30493-0.91704-1.24213-0.61000.h5\n","Epoch 18/20\n","34/34 [==============================] - 112s 3s/step - loss: 0.3155 - categorical_accuracy: 0.9079 - val_loss: 0.8663 - val_categorical_accuracy: 0.6900\n","\n","Epoch 00018: saving model to model_init_2021-06-1820_42_34.362893/model-00018-0.32052-0.91101-0.86627-0.69000.h5\n","Epoch 19/20\n","34/34 [==============================] - 116s 3s/step - loss: 0.3132 - categorical_accuracy: 0.9195 - val_loss: 0.9788 - val_categorical_accuracy: 0.6400\n","\n","Epoch 00019: saving model to model_init_2021-06-1820_42_34.362893/model-00019-0.30492-0.91780-0.97879-0.64000.h5\n","Epoch 20/20\n","34/34 [==============================] - 115s 3s/step - loss: 0.3011 - categorical_accuracy: 0.9249 - val_loss: 0.9169 - val_categorical_accuracy: 0.6600\n","\n","Epoch 00020: saving model to model_init_2021-06-1820_42_34.362893/model-00020-0.30914-0.91403-0.91690-0.66000.h5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V6emlW5jiRSJ"},"source":["### **Loading Final Model**"]},{"cell_type":"code","metadata":{"id":"xyU7twqmAhuu"},"source":["import time\n","from keras.models import load_model\n","model = load_model('model_init_2021-06-1820_42_34.362893/model-00020-0.30914-0.91403-0.91690-0.66000.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WErZ9G-rALPM"},"source":[""],"execution_count":null,"outputs":[]}]}